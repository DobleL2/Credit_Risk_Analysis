# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QYhqRyI-8-hNuKMDT4Rya0i4i8QmJJqP
"""

!pip install boto3
!pip install lightgbm

import json
import os
import time
import boto3
import numpy as np
import lightgbm as lgb
import pandas as pd
import zipfile
from pathlib import Path
import cloudpickle

import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
import joblib

# Preprocess
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.model_selection import train_test_split

model_p = lgb.LGBMClassifier(
    boosting_type='gbdt',
    class_weight=None,
    colsample_bytree=1.0,
    importance_type='split',
    learning_rate=0.1,
    max_depth=-1,
    min_child_samples=20,
    min_child_weight=0.001,
    min_split_gain=0.0,
    n_estimators=100,
    n_jobs=-1,
    num_leaves=31,
    objective=None,
    random_state=42,
    reg_alpha=0.0,
    reg_lambda=0.0,
    subsample=1.0,
    subsample_for_bin=200000,
    subsample_freq=0
)

class_name = None
pred_probability = None
#----

# Extraccion_datos
#----------------------
# Access keys
ACCESS_KEY = 'AKIA2JHUK4EGBAMYAYFY'
SECRET_KEY = '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'
# S3 client
s3 = boto3.client('s3', aws_access_key_id=ACCESS_KEY, aws_secret_access_key=SECRET_KEY)
# S3 data
bucket_name = 'anyoneai-datasets'
# File list of Dataset: s3://anyoneai-datasets/credit-data-2010/
files_to_download = [
    'credit-data-2010/PAKDD2010_Modeling_Data.txt',
    'credit-data-2010/PAKDD2010_Prediction_Data.txt',
    'credit-data-2010/PAKDD2010_VariablesList.XLS',
]
# Download files
try:
    for file_key in files_to_download:
        file_name = file_key.split('/')[-1]  # Extracting file name from the key
        s3.download_file(bucket_name, file_key, file_name)
        print(f"Downloaded {file_name}")
except Exception as e:
    print("Error:", e)
# Data definition
#---------------------
file_path_3 = 'PAKDD2010_Modeling_Data.txt'
mod = pd.read_csv(file_path_3, delimiter='\t', header=None, encoding='latin1')
names = pd.read_excel('PAKDD2010_VariablesList.XLS')
def clean_data_predict(df):
    col_names = names['Var_Title'].to_list()
    col_names[43] = 'EDUCATION_LEVEL_2'
    col_names[53] = 'TARGET'
    df.columns = col_names
    # Columns to eliminate
    columns_to_exclude = ['QUANT_ADDITIONAL_CARDS', 'EDUCATION_LEVEL', 'FLAG_HOME_ADDRESS_DOCUMENT',
                            'FLAG_RG', 'FLAG_CPF', 'FLAG_INCOME_PROOF','FLAG_EMAIL',
                            'CLERK_TYPE','POSTAL_ADDRESS_TYPE','FLAG_MOBILE_PHONE',
                            'COMPANY','MONTHS_IN_THE_JOB','MATE_PROFESSION_CODE','EDUCATION_LEVEL_2',
                            'FLAG_ACSP_RECORD','PROFESSIONAL_CITY','PROFESSIONAL_BOROUGH','PROFESSIONAL_STATE',
                            'PROFESSIONAL_ZIP_3','QUANT_SPECIAL_BANKING_ACCOUNTS',
                            'PROFESSIONAL_PHONE_AREA_CODE','RESIDENCIAL_PHONE_AREA_CODE']
    filtered_df = df.drop(columns=columns_to_exclude)
    # Columns to balance
    columns_to_balance = ['PROFESSION_CODE', 'OCCUPATION_TYPE','RESIDENCE_TYPE','MONTHS_IN_RESIDENCE']
    for column in columns_to_balance:
        filtered_df[column] = filtered_df[column].replace(0, np.nan)
        mean_value = filtered_df[column].mean()
        filtered_df[column] = filtered_df[column].fillna(round(mean_value,0))
    #Error in this columns
    filtered_df['RESIDENCIAL_ZIP_3'] = filtered_df['RESIDENCIAL_ZIP_3'].replace('#DIV/0!', 0)
    filtered_df['RESIDENCIAL_ZIP_3'] = filtered_df['RESIDENCIAL_ZIP_3'].astype(int)
    #Balance age
    valid_age_data = filtered_df[(filtered_df['AGE'] > 14) & (filtered_df['AGE'] < 100)]
    #Clean SEX
    valid_age_data['SEX'] = valid_age_data['SEX'].replace({' ': 'N'})
    valid_df = valid_age_data[valid_age_data['SEX'] != 'N']
    # Function to determine if any card is present
    def has_card(row):
        return 1 if row['FLAG_VISA'] == 1 or row['FLAG_MASTERCARD'] == 1 or row['FLAG_DINERS'] == 1 or row['FLAG_AMERICAN_EXPRESS'] == 1 or row['FLAG_OTHER_CARDS'] == 1 else 0
    # Create a new column 'HAS_ANY_CARD' based on the conditions
    valid_df['HAS_ANY_CARD'] = valid_df.apply(has_card, axis=1)
    # Drop individual card columns if needed
    valid_df = valid_df.drop(['FLAG_VISA', 'FLAG_MASTERCARD', 'FLAG_DINERS', 'FLAG_AMERICAN_EXPRESS', 'FLAG_OTHER_CARDS'], axis=1)
    #Upper categories
    upper_columns = ['RESIDENCIAL_BOROUGH', 'CITY_OF_BIRTH', 'RESIDENCIAL_BOROUGH']
    for col in upper_columns:
        valid_df[col] = valid_df[col].apply(lambda x: x.upper() if isinstance(x, str) else x)
    #Replace some values
    valid_df['CITY_OF_BIRTH'] = valid_df['CITY_OF_BIRTH'].replace({' ': 'NULL'})
    valid_df['STATE_OF_BIRTH'] = valid_df['STATE_OF_BIRTH'].replace({' ': 'NULL'})
    valid_df['FLAG_RESIDENCIAL_PHONE'] = valid_df['FLAG_RESIDENCIAL_PHONE'].replace({'Y':1,'N':0})
    valid_df['FLAG_PROFESSIONAL_PHONE'] = valid_df['FLAG_PROFESSIONAL_PHONE'].replace({'Y':1,'N':0})
    valid_df = valid_df.drop(['ID_CLIENT','APPLICATION_SUBMISSION_TYPE','STATE_OF_BIRTH', 'CITY_OF_BIRTH', 'RESIDENCIAL_CITY','RESIDENCIAL_STATE','RESIDENCIAL_BOROUGH'],axis = 1)
    return valid_df
df_clean = clean_data_predict(mod)

X_train, X_test, y_train, y_test = train_test_split(df_clean.drop(['TARGET'],axis = 1), df_clean["TARGET"], test_size=0.2, random_state=42)
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.base import BaseEstimator, TransformerMixin
import joblib

class DataFrameSelector(BaseEstimator, TransformerMixin):
    def __init__(self, attribute_names):
        self.attribute_names = attribute_names

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return X[self.attribute_names].values

def preprocess_and_save_pipeline(train_df, save_path='preprocess_pipeline.joblib'):
    categorical_cols = train_df.select_dtypes(include=['object']).columns.tolist()
    numerical_cols = train_df.select_dtypes(exclude=['object']).columns.tolist()

    # Initialize lists for binary and non-binary categorical columns
    binary_categorical_cols = []
    non_binary_categorical_cols = []

    # Iterate through the categorical columns
    for col in categorical_cols:
        unique_values = train_df[col].nunique()
        if unique_values == 2:
            binary_categorical_cols.append(col)
        else:
            non_binary_categorical_cols.append(col)

    # Define the pipeline steps
    binary_categorical_pipeline = Pipeline([
        ('selector', DataFrameSelector(attribute_names=binary_categorical_cols)),
        ('ordinal_encoder', OrdinalEncoder())
    ])

    non_binary_categorical_pipeline = Pipeline([
        ('selector', DataFrameSelector(attribute_names=non_binary_categorical_cols)),
        ('onehot_encoder', OneHotEncoder(sparse=False, handle_unknown='ignore'))
    ])

    numerical_pipeline = Pipeline([
        ('selector', DataFrameSelector(attribute_names=numerical_cols)),
        ('imputer', SimpleImputer(strategy='median')),
        ('scaler', StandardScaler())
    ])

    # Combine all pipelines using ColumnTransformer
    preprocess_pipeline = ColumnTransformer([
        ('binary_categorical', binary_categorical_pipeline, binary_categorical_cols),
        ('non_binary_categorical', non_binary_categorical_pipeline, non_binary_categorical_cols),
        ('numerical', numerical_pipeline, numerical_cols)
    ])

    # Fit and transform the training data
    train_processed = preprocess_pipeline.fit_transform(train_df)

    # Convert the processed array back to a DataFrame
    train_processed_df = pd.DataFrame(train_processed, columns=binary_categorical_cols + non_binary_categorical_cols + numerical_cols)

    # Save the pipeline to a joblib file
    joblib.dump(preprocess_pipeline, save_path)

    print("Preprocessing completed, and pipeline saved to:", save_path)

    return train_processed_df

train_processed_df = preprocess_and_save_pipeline(X_train)
model_fit = model_p.fit(train_processed_df, y_train)
joblib.dump(model_fit, 'model_fit.joblib')

dict_features = {'PAYMENT_DAY': [14], 'SEX': ['M'],
                 'MARITAL_STATUS': [0.32316679991959296],
                 'QUANT_DEPENDANTS': [1.9077678811444132],
                 'NACIONALITY': [0.22034757035662245],
                 'FLAG_RESIDENCIAL_PHONE': [1],
                 'RESIDENCE_TYPE': [1.7495547046722986],
                 'MONTHS_IN_RESIDENCE': [48.20525143724116],
                 'PERSONAL_MONTHLY_INCOME': [896524.4425945266],
                 'OTHER_INCOMES': [317.9287421872096],
                 'QUANT_BANKING_ACCOUNTS': [0.22551415109553608],
                 'PERSONAL_ASSETS_VALUE': [8712.210598266734],
                 'QUANT_CARS': [1.3497474993388459],
                 'FLAG_PROFESSIONAL_PHONE': [1],
                 'PROFESSION_CODE': [5.172599117336276],
                 'OCCUPATION_TYPE': [3.7989584602272704],
                 'PRODUCT': [1.4676470986579082],
                 'AGE': [36.037719317278004],
                 'RESIDENCIAL_ZIP_3': [250.58099041358662],
                 'HAS_ANY_CARD': [0]}
datos = pd.DataFrame(dict_features)
#Preprocess
pipeline = joblib.load('preprocess_pipeline.joblib')
data_new = pipeline.transform(datos)

model = joblib.load('model_fit.joblib')
pred = model.predict(data_new)
print(pred)

prediction_proba = model.predict_proba(data_new)
positive_class_proba = 1 - prediction_proba[:, 1]
print("Probability of the positive class for new data:", positive_class_proba)